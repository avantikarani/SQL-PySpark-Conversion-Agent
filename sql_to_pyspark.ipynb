{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61290160-5a6d-4744-9d59-ce18d5c7468e",
   "metadata": {},
   "source": [
    "**Install dependency libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761404c2-b32f-47c1-b16d-a8cd4ce08c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q google-adk google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b395e-0c9e-4698-ba72-6913dfd317e6",
   "metadata": {},
   "source": [
    "**Configure Gemini API Key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a9d5ad-608c-4b2d-8036-37d52a4bca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"GOOGLE_GENAI_USE_VERTEXAI\", \"FALSE\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = 'GOOGLE-API-KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda5bd7-b487-437a-b9b2-08d9158c4077",
   "metadata": {},
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d91d2b-2d0c-4b76-a6fb-72fc6cae3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "from google.adk.agents import LlmAgent, SequentialAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c93a6-26ab-4cf9-8e82-aa103a1115d9",
   "metadata": {},
   "source": [
    "**Configuring Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a712d0b-01e9-4200-abff-e56d3cc654c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"sql_to_pyspark_app\"\n",
    "USER_ID = \"demo_user\"\n",
    "SESSION_ID = \"sql_to_pyspark_session\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a4d7c-2c22-4249-9411-8f6e7bc5ab87",
   "metadata": {},
   "source": [
    "**SQL Normalizer Agent** - Clean and standardize the SQL code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ff764c-c35e-412b-8dee-ee9563566381",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_normalizer_agent = LlmAgent(\n",
    "    name=\"SQL_Normalizer\",\n",
    "    model='gemini-2.5-flash-lite',\n",
    "    instruction=(\n",
    "        \"You are a SQL normalization assistant.\\n\"\n",
    "        \"- Input: raw SQL query from the user.\\n\"\n",
    "        \"- Output: the same query, but formatted and canonicalized.\\n\"\n",
    "        \"- Do NOT explain anything. Return ONLY the SQL text.\\n\"\n",
    "    ),\n",
    "    description=\"Normalizes user SQL into a clean canonical form.\",\n",
    "    output_key=\"normalized_sql\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31e73a-2ad2-4985-80d5-ee56ae4cfb86",
   "metadata": {},
   "source": [
    "**PySpark Conversion Agent** - Converts SQL into PySpark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c267a1-33a3-46c5-862c-62bba3564507",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_to_pyspark_agent = LlmAgent(\n",
    "    name=\"SQL_to_PySpark_Converter\",\n",
    "    model='gemini-2.5-flash-lite',\n",
    "    instruction=(\n",
    "        \"You convert SQL queries into equivalent PySpark DataFrame code.\\n\\n\"\n",
    "        \"Input:\\n\"\n",
    "        \"  A normalized SQL query is available as {normalized_sql} in the state.\\n\\n\"\n",
    "        \"General rules:\\n\"\n",
    "        \"  - Always include necessary imports at the top:\\n\"\n",
    "        \"        from pyspark.sql import functions as F\\n\"\n",
    "        \"        from pyspark.sql import SparkSession\\n\\n\"\n",
    "        \"  - Assume a SparkSession named 'spark' already exists.\\n\"\n",
    "        \"  - Use spark.table(\\\"<db.table>\\\") for each base table in FROM/JOIN.\\n\"\n",
    "        \"  - Translate WHERE, SELECT, GROUP BY, HAVING, ORDER BY, JOIN into PySpark DataFrame API.\\n\"\n",
    "        \"  - Prefer method chaining on DataFrames.\\n\"\n",
    "        \"  - Use variable name 'final_df' for the final resulting DataFrame.\\n\\n\"\n",
    "        \"Conditional logic (IMPORTANT):\\n\"\n",
    "        \"  - For SQL CASE WHEN / THEN / ELSE expressions, map them to PySpark using F.when / .otherwise.\\n\"\n",
    "        \"    Example:\\n\"\n",
    "        \"       CASE WHEN status = 'ACTIVE' THEN 1 ELSE 0 END AS is_active\\n\"\n",
    "        \"    should become something like:\\n\"\n",
    "        \"       df = df.withColumn(\\n\"\n",
    "        \"           'is_active',\\n\"\n",
    "        \"           F.when(F.col('status') == 'ACTIVE', F.lit(1)).otherwise(F.lit(0))\\n\"\n",
    "        \"       )\\n\\n\"\n",
    "        \"  - For multiple WHEN branches, chain F.when calls:\\n\"\n",
    "        \"       CASE\\n\"\n",
    "        \"         WHEN score >= 90 THEN 'A'\\n\"\n",
    "        \"         WHEN score >= 75 THEN 'B'\\n\"\n",
    "        \"         ELSE 'C'\\n\"\n",
    "        \"       END AS grade\\n\"\n",
    "        \"    ->\\n\"\n",
    "        \"       df = df.withColumn(\\n\"\n",
    "        \"           'grade',\\n\"\n",
    "        \"           F.when(F.col('score') >= 90, F.lit('A'))\\n\"\n",
    "        \"            .when(F.col('score') >= 75, F.lit('B'))\\n\"\n",
    "        \"            .otherwise(F.lit('C'))\\n\"\n",
    "        \"       )\\n\\n\"\n",
    "        \"  - For SQL IF(condition, a, b) expressions, also use F.when(condition, a).otherwise(b).\\n\\n\"\n",
    "        \"Output format:\\n\"\n",
    "        \"  - Return ONLY valid Python code in a single ```python ... ``` block.\\n\"\n",
    "        \"  - No extra explanation or markdown outside the code block.\\n\"\n",
    "    ),\n",
    "    description=\"Converts normalized SQL into PySpark DataFrame code, including CASE using when/otherwise.\",\n",
    "    output_key=\"pyspark_code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a563c9-4ce8-4da0-b863-38f85330fb99",
   "metadata": {},
   "source": [
    "**Sequential Agent** - for ordered exexution of Normalizer -> Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88a73a7-bbc0-484e-8113-d572fe1a9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_to_pyspark_pipeline = SequentialAgent(\n",
    "    name=\"SQL_to_PySpark_Pipeline\",\n",
    "    sub_agents=[sql_normalizer_agent, sql_to_pyspark_agent],\n",
    "    description=\"Pipeline: normalize SQL, then convert to PySpark.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e91efc-f14e-4e62-8d26-3b7f405f7b4c",
   "metadata": {},
   "source": [
    "**Initialize Memory Service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6349f772-23dd-45e5-b4c8-0d31f25b6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Service Session\n",
    "session_service = InMemorySessionService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e8f1e9-0e62-4b71-8b85-89b8aad991d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    ")\n",
    "\n",
    "#Create Runner\n",
    "runner = Runner(\n",
    "    agent=sql_to_pyspark_pipeline,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f1178b6-6158-409c-a2d5-3a6098834ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sql_to_pyspark(sql_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Run the sequential agent pipeline on the given SQL and return PySpark code.\n",
    "    \"\"\"\n",
    "    content = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=sql_query)],\n",
    "    )\n",
    "\n",
    "    final_text = ''\n",
    "\n",
    "    for event in runner.run(\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_ID,\n",
    "        new_message=content,\n",
    "    ):\n",
    "        if event.is_final_response():\n",
    "            # The final response comes from the last sub-agent in the sequence\n",
    "            if event.content and event.content.parts:\n",
    "                final_text = event.content.parts[0].text\n",
    "\n",
    "    if not final_text:\n",
    "        raise RuntimeError(\"Agent did not return any final text response.\")\n",
    "\n",
    "    # Strip code fences if present\n",
    "    if \"```\" in final_text:\n",
    "        lines = []\n",
    "        for line in final_text.splitlines():\n",
    "            if line.strip().startswith(\"```\"):\n",
    "                continue\n",
    "            lines.append(line)\n",
    "        final_text = \"\\n\".join(lines).strip()\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf032a8-c7fa-4183-ae53-420827814595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_pyspark_rule_based(sql_query: str, pyspark_code: str, must_contain: List[str], nice_to_have: List[str] | None = None,) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - must_contain: list of substrings that MUST appear in the PySpark code\n",
    "    - nice_to_have: optional list of substrings that give bonus score if present\n",
    "    \"\"\"\n",
    "    if nice_to_have is None:\n",
    "        nice_to_have = []\n",
    "\n",
    "    missing_required = [pattern for pattern in must_contain if pattern not in pyspark_code]\n",
    "    matched_required = len(must_contain) - len(missing_required)\n",
    "\n",
    "    matched_optional = [pattern for pattern in nice_to_have if pattern in pyspark_code]\n",
    "\n",
    "    # Simple scoring:\n",
    "    # - semantic_correctness ≈ how many required patterns matched\n",
    "    # - syntactic_validity: crude check that we see \"df =\" and \"spark.table(\"\n",
    "    # - readability: crude check for chaining / withColumn usage\n",
    "    \n",
    "    total_required = max(len(must_contain), 1)\n",
    "    semantic_score = int(10 * matched_required / total_required)\n",
    "\n",
    "    syntactic_score = 0\n",
    "    if \"spark.table(\" in pyspark_code:\n",
    "        syntactic_score = 7\n",
    "    if \"spark.table(\" in pyspark_code and \"groupBy(\" in pyspark_code:\n",
    "        syntactic_score = 9\n",
    "\n",
    "    readability_score = 0\n",
    "    if \".groupBy(\" in pyspark_code or \".withColumn(\" in pyspark_code:\n",
    "        readability_score = 7\n",
    "    if \".groupBy(\" in pyspark_code and \".agg(\" in pyspark_code:\n",
    "        readability_score = 9\n",
    "\n",
    "    # Small bonus for nice-to-have patterns\n",
    "    bonus = min(len(matched_optional), 2)\n",
    "    overall_score = max(0, min(10, int((semantic_score + syntactic_score + readability_score) / 3) + bonus))\n",
    "\n",
    "    comments = []\n",
    "    if missing_required:\n",
    "        comments.append(f\"Missing expected patterns: {missing_required}\")\n",
    "    if matched_optional:\n",
    "        comments.append(f\"Good: found optional patterns {matched_optional}\")\n",
    "    if not comments:\n",
    "        comments.append(\"Looks structurally OK based on rule-based checks.\")\n",
    "\n",
    "    return {\n",
    "        \"semantic_correctness\": semantic_score,\n",
    "        \"syntactic_validity\": syntactic_score,\n",
    "        \"readability\": readability_score,\n",
    "        \"overall_score\": overall_score,\n",
    "        \"comments\": \" | \".join(comments),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a6f4dd3-0c4c-401c-85c5-f7464d811acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_to_pyspark_tests(test_cases: List[Dict[str, Any]],) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run SQL → PySpark → rule-based evaluation for one or many test cases.\n",
    "\n",
    "    test_cases example:\n",
    "        [\n",
    "            {\n",
    "                \"name\": \"case1\",\n",
    "                \"sql\": \"...\",\n",
    "                \"must_contain\": [\"groupBy(\", \".agg(\"],\n",
    "                \"nice_to_have\": [\"F.when(\"],\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    for case in test_cases:\n",
    "        name = case.get(\"name\", \"unnamed_case\")\n",
    "        sql = case[\"sql\"]\n",
    "        must_contain = case.get(\"must_contain\", [])\n",
    "        nice_to_have = case.get(\"nice_to_have\", [])\n",
    "\n",
    "        # 1) Convert SQL → PySpark via your main agent\n",
    "        pyspark_code = convert_sql_to_pyspark(sql)\n",
    "\n",
    "        # 2) Evaluate using rule-based checks ONLY (no Gemini)\n",
    "        evaluation = _evaluate_pyspark_rule_based(\n",
    "            sql_query=sql,\n",
    "            pyspark_code=pyspark_code,\n",
    "            must_contain=must_contain,\n",
    "            nice_to_have=nice_to_have,\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"sql\": sql,\n",
    "                \"pyspark_code\": pyspark_code,\n",
    "                \"evaluation\": evaluation,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d414c1-3593-44b8-a67d-f4c7f46e6526",
   "metadata": {},
   "source": [
    "**Test Cases** for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfab1ef6-1781-486f-9203-a7129026beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"simple_where\",\n",
    "        \"sql\": \"\"\"\n",
    "            SELECT\n",
    "                id,\n",
    "                name,\n",
    "                country\n",
    "            FROM sales.customers\n",
    "            WHERE country = 'IN';\n",
    "        \"\"\",\n",
    "        \"must_contain\": [\n",
    "            \"spark.table(\\\"sales.customers\\\")\",\n",
    "            \".filter(\",\n",
    "        ],\n",
    "        \"nice_to_have\": [\n",
    "            \"F.col(\\\"country\\\")\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"group_by_agg_having\",\n",
    "        \"sql\": \"\"\"\n",
    "            SELECT\n",
    "                country,\n",
    "                COUNT(*) AS customer_count,\n",
    "                AVG(age) AS avg_age\n",
    "            FROM sales.customers\n",
    "            WHERE status = 'ACTIVE'\n",
    "            GROUP BY country\n",
    "            HAVING COUNT(*) >= 100\n",
    "            ORDER BY customer_count DESC;\n",
    "        \"\"\",\n",
    "        \"must_contain\": [\n",
    "            \"spark.table(\\\"sales.customers\\\")\",\n",
    "            \".filter(\",\n",
    "            \".groupBy(\",\n",
    "            \".agg(\",\n",
    "            \".orderBy(\",\n",
    "        ],\n",
    "        \"nice_to_have\": [\n",
    "            \"F.count(\",\n",
    "            \"F.avg(\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"join_inner_group_by\",\n",
    "        \"sql\": \"\"\"\n",
    "            SELECT\n",
    "                c.country,\n",
    "                COUNT(DISTINCT c.id) AS active_customers,\n",
    "                SUM(o.amount) AS total_amount\n",
    "            FROM sales.customers c\n",
    "            JOIN sales.orders o\n",
    "              ON c.id = o.customer_id\n",
    "            WHERE o.status = 'COMPLETED'\n",
    "              AND o.order_date >= DATE '2024-01-01'\n",
    "            GROUP BY c.country\n",
    "            HAVING SUM(o.amount) > 100000\n",
    "            ORDER BY total_amount DESC;\n",
    "        \"\"\",\n",
    "        \"must_contain\": [\n",
    "            \"spark.table(\\\"sales.customers\\\")\",\n",
    "            \"spark.table(\\\"sales.orders\\\")\",\n",
    "            \".join(\",\n",
    "            \".groupBy(\",\n",
    "            \".agg(\",\n",
    "        ],\n",
    "        \"nice_to_have\": [\n",
    "            \"F.sum(\",\n",
    "            \"F.countDistinct(\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"case_when_flag\",\n",
    "        \"sql\": \"\"\"\n",
    "            SELECT\n",
    "                id,\n",
    "                status,\n",
    "                CASE\n",
    "                    WHEN status = 'ACTIVE' THEN 1\n",
    "                    ELSE 0\n",
    "                END AS is_active\n",
    "            FROM sales.customers;\n",
    "        \"\"\",\n",
    "        \"must_contain\": [\n",
    "            \"spark.table(\\\"sales.customers\\\")\",\n",
    "            \".withColumn(\",\n",
    "            \"F.when(\",\n",
    "            \".otherwise(\",\n",
    "        ],\n",
    "        \"nice_to_have\": [\n",
    "            \"F.col(\\\"status\\\")\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"case_when_multi_segment\",\n",
    "        \"sql\": \"\"\"\n",
    "            SELECT\n",
    "                customer_id,\n",
    "                CASE\n",
    "                    WHEN total_amount >= 10000 THEN 'HIGH'\n",
    "                    WHEN total_amount >= 5000 THEN 'MEDIUM'\n",
    "                    ELSE 'LOW'\n",
    "                END AS customer_segment\n",
    "            FROM sales.customer_agg;\n",
    "        \"\"\",\n",
    "        \"must_contain\": [\n",
    "            \"spark.table(\\\"sales.customer_agg\\\")\",\n",
    "            \".withColumn(\",\n",
    "            \"F.when(\",\n",
    "            \".otherwise(\",\n",
    "        ],\n",
    "        \"nice_to_have\": [\n",
    "            \".when(\",\n",
    "            \"F.col(\\\"total_amount\\\")\",\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45155c7-4b7f-4dfb-8485-48c33bc0e47f",
   "metadata": {},
   "source": [
    "***Run the tests***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61da94c1-9930-49cf-bcf2-506a00ffa044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Test case: simple_where\n",
      "\n",
      "Generated PySpark Code:\n",
      " from pyspark.sql import functions as F\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.appName(\"sql_to_pyspark\").getOrCreate()\n",
      "\n",
      "customers_df = spark.table(\"sales.customers\")\n",
      "\n",
      "final_df = customers_df.filter(F.col(\"country\") == \"IN\") \\\n",
      "    .select(\"id\", \"name\", \"country\")\n",
      "\n",
      "final_df.show()\n",
      "\n",
      "Evaluation:\n",
      " {'semantic_correctness': 10, 'syntactic_validity': 7, 'readability': 0, 'overall_score': 6, 'comments': 'Good: found optional patterns [\\'F.col(\"country\")\\']'}\n",
      "================================================================================\n",
      "Test case: group_by_agg_having\n",
      "\n",
      "Generated PySpark Code:\n",
      " from pyspark.sql import functions as F\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.appName(\"sql_to_pyspark\").getOrCreate()\n",
      "\n",
      "customers_df = spark.table(\"sales.customers\")\n",
      "\n",
      "final_df = customers_df.filter(F.col(\"status\") == \"ACTIVE\") \\\n",
      "    .groupBy(\"country\") \\\n",
      "    .agg(\n",
      "        F.count(\"*\").alias(\"customer_count\"),\n",
      "        F.avg(\"age\").alias(\"avg_age\")\n",
      "    ) \\\n",
      "    .filter(F.col(\"customer_count\") >= 100) \\\n",
      "    .orderBy(F.col(\"customer_count\").desc())\n",
      "\n",
      "final_df.show()\n",
      "\n",
      "Evaluation:\n",
      " {'semantic_correctness': 10, 'syntactic_validity': 9, 'readability': 9, 'overall_score': 10, 'comments': \"Good: found optional patterns ['F.count(', 'F.avg(']\"}\n",
      "================================================================================\n",
      "Test case: join_inner_group_by\n",
      "\n",
      "Generated PySpark Code:\n",
      " from pyspark.sql import functions as F\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.appName(\"sql_to_pyspark\").getOrCreate()\n",
      "\n",
      "customers_df = spark.table(\"sales.customers\")\n",
      "orders_df = spark.table(\"sales.orders\")\n",
      "\n",
      "joined_df = customers_df.alias(\"c\").join(\n",
      "    orders_df.alias(\"o\"),\n",
      "    F.col(\"c.id\") == F.col(\"o.customer_id\")\n",
      ")\n",
      "\n",
      "filtered_df = joined_df.filter(\n",
      "    (F.col(\"o.status\") == \"COMPLETED\") &\n",
      "    (F.col(\"o.order_date\") >= F.lit(\"2024-01-01\"))\n",
      ")\n",
      "\n",
      "grouped_df = filtered_df.groupBy(\"c.country\") \\\n",
      "    .agg(\n",
      "        F.countDistinct(\"c.id\").alias(\"active_customers\"),\n",
      "        F.sum(\"o.amount\").alias(\"total_amount\")\n",
      "    )\n",
      "\n",
      "having_df = grouped_df.filter(F.col(\"total_amount\") > 100000)\n",
      "\n",
      "final_df = having_df.orderBy(F.col(\"total_amount\").desc())\n",
      "\n",
      "final_df.show()\n",
      "\n",
      "Evaluation:\n",
      " {'semantic_correctness': 10, 'syntactic_validity': 9, 'readability': 9, 'overall_score': 10, 'comments': \"Good: found optional patterns ['F.sum(', 'F.countDistinct(']\"}\n",
      "================================================================================\n",
      "Test case: case_when_flag\n",
      "\n",
      "Generated PySpark Code:\n",
      " from pyspark.sql import functions as F\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.appName(\"sql_to_pyspark\").getOrCreate()\n",
      "\n",
      "customers_df = spark.table(\"sales.customers\")\n",
      "\n",
      "final_df = customers_df.withColumn(\n",
      "    'is_active',\n",
      "    F.when(F.col('status') == 'ACTIVE', F.lit(1)).otherwise(F.lit(0))\n",
      ")\n",
      "\n",
      "final_df.show()\n",
      "\n",
      "Evaluation:\n",
      " {'semantic_correctness': 10, 'syntactic_validity': 7, 'readability': 7, 'overall_score': 8, 'comments': 'Looks structurally OK based on rule-based checks.'}\n",
      "================================================================================\n",
      "Test case: case_when_multi_segment\n",
      "\n",
      "Generated PySpark Code:\n",
      " from pyspark.sql import functions as F\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.appName(\"sql_to_pyspark\").getOrCreate()\n",
      "\n",
      "customer_agg_df = spark.table(\"sales.customer_agg\")\n",
      "\n",
      "final_df = customer_agg_df.withColumn(\n",
      "    'customer_segment',\n",
      "    F.when(F.col('total_amount') >= 10000, F.lit('HIGH'))\n",
      "     .when(F.col('total_amount') >= 5000, F.lit('MEDIUM'))\n",
      "     .otherwise(F.lit('LOW'))\n",
      ").select(\"customer_id\", \"customer_segment\")\n",
      "\n",
      "final_df.show()\n",
      "\n",
      "Evaluation:\n",
      " {'semantic_correctness': 10, 'syntactic_validity': 7, 'readability': 7, 'overall_score': 9, 'comments': \"Good: found optional patterns ['.when(']\"}\n"
     ]
    }
   ],
   "source": [
    "all_results = run_sql_to_pyspark_tests(test_cases)\n",
    "\n",
    "for res in all_results:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Test case:\", res[\"name\"])\n",
    "    print(\"\\nGenerated PySpark Code:\\n\", res[\"pyspark_code\"])\n",
    "    print(\"\\nEvaluation:\\n\", res[\"evaluation\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
